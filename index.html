<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="AttentionHand is accepted by ECCV 2024.">
  <meta name="keywords" content="AttentionHand>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  
  <link rel="stylesheet" href="./static/css/twentytwenty.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script src="./static/js/jquery.twentytwenty.js"></script>


</head>
<body>
<!-- 
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=1OSw79kAAAAJ&hl=ko">Junho Park</a><sup>1,2*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=1OSw79kAAAAJ&hl=ko">Kyeongbo Kong</a><sup>3*</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=3WYxpuYAAAAJ&hl=ko&oi=ao">Sukju Kang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Sogang University,</span>
            <span class="author-block"><sup>2</sup>LG Electronics,</span>
            <span class="author-block"><sup>3</sup>Pusan National University</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">ECCV 2024</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/redorangeyellowy/AttentionHand"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">* : Equal Contribution</span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img id="teaser" src="./static/images/main_fig1_intro.png" alt="Main Image" style="width: 100%; height: auto;">
        <h2 class="subtitle has-text-centered">
          <span class="dnerf">AttentionHand</span> is a novel method for text-driven controllable hand image generation.
          (1) In the data preparation phase, we prepare global and local RGB images, global and local hand mesh images, bounding box, and text prompt.
          (2) In the encoding phase, we get global and local latent image embeddings, and text embedding.
          (3) In the conditioning phase, we refine image embeddings through the text attention stage, and obtain the diffusion feature through the visual attention stage.
          (4) In the decoding phase, we generate a new hand image from the diffusion feature.
        </h2>
      </div>
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered">
        <div class="column is-full-width">
            <video autoplay muted loop playsinline style="width: 65%; height: auto; display: block; margin: auto;">
              <source src="./static/videos/1.mp4"
                      type="video/mp4">
            </video>
            <p class="subtitle has-text-centered" style="margin-top: 1.5rem;">"Generate a fully furnished living room whose width, length, height are each 4.5m, 3.3m, 2m. The walls are covered with dark grey rectangular tiles, and there are two windows and a door on the walls."</p>
  
            <video autoplay muted loop playsinline style="width: 65%; height: auto; display: block; margin: auto;">
              <source src="./static/videos/2.mp4"
                      type="video/mp4">
            </video>
            <p class="subtitle has-text-centered" style="margin-top: 1.5rem;">"Generate a living room whose width and length are each 5.1m, 3.8m. In the room, there 2 windows and 1 door. The pattern of the walls are floral, and the overall color is baby pink."</p>

            <video autoplay muted loop playsinline style="width: 65%; height: auto; display: block; margin: auto;">
              <source src="./static/videos/3.mp4"
                      type="video/mp4">
            </video>
            <p class="subtitle has-text-centered" style="margin-top: 1.5rem;">"Generate a bedroom whose width and length are each 4.4m and 4.2m. In the room, there are two windows and a door. The walls are painted in white, and the floor is wooden."</p>
          </div>
        </div>
    </div> 
  </section>  




  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body video-hero-body">
        
        <video id="teaser" autoplay muted loop playsinline height="auto" width="50%">
          <source src="./static/videos/1.mp4"
                  type="video/mp4">
        </video>
        <p class="subtitle has-text-centered">"Generate a fully furnished living room whose width, length, height are each 4.5m, 3.3m, 2m. The walls are covered with dark grey rectangular tiles, and there are two windows and a door on the walls."</p>
      </div>

      <div class="hero-body video-hero-body">
        <video id="teaser" autoplay muted loop playsinline height="auto" width="50%">
          <source src="./static/videos/2.mp4"
                  type="video/mp4">
        </video>
        <p class="subtitle has-text-centered">"Generate a living room whose width and length are each 5.1m, 3.8m. In the room, there 2 windows and 1 door. The pattern of the walls are floral, and the overall color is baby pink."</p>
      </div>

      <div class="hero-body video-hero-body">
        <video id="teaser" autoplay muted loop playsinline height="auto" width="50%">
          <source src="./static/videos/3.mp4"
                  type="video/mp4">
        </video>
        <p class="subtitle has-text-centered">"Generate a bedroom whose width and length are each 4.4m and 4.2m. In the room, there are two windows and a door. The walls are painted in white, and the floor is wooden."</p>
      </div>
    </div>
  </section> -->
  
<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class = "item">
          <video poster="" id="1" autoplay controls muted loop playsinline height="50%">
            <source src="./static/videos/1.mp4"
                    type="video/mp4">
          </video> -->
        <!-- </div>
        <div class="item">
          <video poster="" id="2" autoplay controls muted loop playsinline height="20%">
            <source src="./static/videos/2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="3" autoplay controls muted loop playsinline height="20%">
            <source src="./static/videos/3.mp4"
                    type="video/mp4">
          </video>
        </div> -->
        <!-- <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div> -->
      <!-- </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recently, there has been a significant amount of research conducted on 3D hand reconstruction to use various forms of human-computer interaction. However, 3D hand reconstruction in the wild is challenging due to extreme lack of in-the-wild 3D hand datasets. Especially, when hands are in complex pose such as interacting hands, the problems like appearance similarity, self-handed occclusion and depth ambiguity make it more difficult. To overcome these issues, we propose AttentionHand, a novel method for text-driven controllable hand image generation. Since AttentionHand can generate various and numerous in-the-wild hand images well-aligned with 3D hand label, we can acquire a new 3D hand dataset, and can relieve the domain gap between indoor and outdoor scenes. Our method needs easy-to-use four modalities (i.e, an RGB image, a hand mesh image from 3D label, a bounding box, and a text prompt). These modalities are embedded into the latent space by the encoding phase. Then, through the text attention stage, hand-related tokens from the given text prompt are attended to highlight hand-related regions of the latent embedding. After the highlighted embedding is fed to the visual attention stage, hand-related regions in the embedding are attended by conditioning global and local hand mesh images with the diffusion-based pipeline. In the decoding phase, the final feature is decoded to new hand images, which are well-aligned with the given hand mesh image and text prompt. As a result, AttentionHand achieved state-of-the-art among text-to-hand image generation models, and the performance of 3D hand mesh reconstruction was improved by additionally training with hand images generated by AttentionHand.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <!-- 3D Room Generation. -->
        <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Text Attention Stage</h2>
          <div class="content has-text-justified">
            <p>
                <i>Programmable-Room</i> can create a textured and fully furnished 3D room mesh from a text instruction (Generating an empty room is also possible).
                Users can specify the room shape and size; the texture of the ceiling, walls, and floor; and furniture. When Generating a textured and furnished 3D room mesh, <i>Programmable-Room</i> uses an LLM to generate python codes like below. 
                The output of each line is visualized along with the corresponding code. 
            </p>
            <img src="./static/images/generating.png" alt="Process Image" style="width: 65%; height: auto; display: block; margin: auto;">
          </div> 

            <div class="content has-text-justified">
                <p>
                In summary, three images (layout image, depth map, semantic map) are generated which follow the user-specified room shape.
                Then, the three images are used as visual prompts for generating a panorama texture image, which is then folded into an empty room mesh.
                After allocating appropriate furniture according to the room type, we gain the textured and fully furnished 3D room mesh.
                </p>            
            </div>

            <div class="content has-text-justified">
                <p>
                    Whereas the state-of-the-art method generates structurally unrealsitic room meshes with repetitive furniture,
                     <i>Programmable-Room</i> creates structurally plausible room meshes whose shape and texture match the instruction.
                </p>
                <img src="./static/images/exp_mesh1.png" alt="Mesh Comparison Image" style="width: 100%; height: auto; display: block; margin: auto;">
              </div>
        </div>
       </div>
          
          <!-- /Generating -->


    <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Visual Attention Stage</h2>
          <div class="content has-text-justified">
            <p>
                For editing tasks, <i>Programmable-Room</i> loads the last room elements, generates new elements according to the new insturction, and combines the new elements with the loaded ones.
                This process is repeated until the user is satisfied. Currently, we support room shape editing, room texture editing, and furniture editing (remove, add, replace). 
            </p>
            <img src="./static/images/editing.png" alt="Editing Image" style="width: 100%; height: auto; display: block; margin: auto;">
           
            </div>
        </div>
       </div>


       <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Optimization</h2>
          <div class="content has-text-justified">
            <p>
                We developed modules for generating and editing room meshes. 
                Users can easily add a new module along with few usage examples for in-context-learning of the LLM, in order to conduct more diverse tasks.
            </p>
            <img src="./static/images/modules.png" alt="Module List Image" style="width: 50%; height: auto; display: block; margin: auto;">
           
            </div>
            <div class="content has-text-justified">
                <p>
                    One of the most import modules is GenTexture since there is no existing method to generate a panorama image of an empty room according to specific <i>texture</i> and, most importantly, <i>shape</i>.
                    For this new task, we developed Panorama Room Image Generation (PRIG), a diffusion-based model that generates a panorama room texture image from text and multiple visual prompts. 
                </p>
                <img src="./static/images/method_pano.png" alt="PRIG Image" style="width: 100%; height: auto;">
               
                </div>
                    
                <div class="content has-text-justified">
                    <p>
                        In comparison with the state-of-the-art methods, PRIG generates images which better reflect the texture information. 
                        Moreover, PRIG generates more structurally coherent images. For example, unlike those of the existing methods, in the images generated by PRIG, the left and the right sides are continuous
                    </p>
                    <img src="./static/images/exp_pano.png" alt="PRIG Comparison Image" style="width: 70%; height: auto; display: block; margin: auto;">
                   
                    </div>
        </div>
       </div>



      <div class="columns is-centered">
        <!-- Diverse Layouts -->
        <div class="column">
            <div class="content">
              <h2 class="title is-3">Diverse Shapes</h2>
              <p>
                One of the benefits of <i>Programmable-Room</i> is the creation of rooms with divese shapes. 
                This is possible due to the new method, we developed, to generate a textured 3D room mesh from user-specified room shapes.
              </p>
              <img src="./static/images/exp_layout.png" alt="Layout Image" style="width: 100%; height: auto;">
            </div>
          </div>
          <!-- /Diverse Layouts -->

      <!-- Complex Instructions. -->
      <div class="column">
        <h2 class="title is-3">Complex Instructions</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              Using predifined modules, <i>Programmable-Room</i> can transfer a complicated instruction of multiple tasks into an arranged python code. 
            </p>
            <img src="./static/images/code.png" alt="Compex Code Image" style="width: 100%; height: auto;">
          </div>

        </div>
      </div>
    </div>
    <!--/ Complexn Instructions. -->
</div>
</section>




<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website source code borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">here</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>



  




</body>
</html>